{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chin_up_Classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arslanali4343/House-Price-Prediction/blob/main/Chin_up_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUqzhFpbqdni"
      },
      "source": [
        "# Correct vs Wrong Chin Up Image Classification\n",
        "## Exercise 1: Building a Convnet from Scratch\n",
        "**_Estimated completion time: 5 to 7 minutes_**\n",
        "\n",
        "In this exercise, we will build a classifier model from scratch that is able to distinguish Correct from Wrong Chin up. We will follow these steps:\n",
        "\n",
        "1. Explore the example data\n",
        "2. Build a small convnet from scratch to solve our classification problem\n",
        "3. Evaluate training and validation accuracy\n",
        "\n",
        "Let's go"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn_kBbwo2epF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eSJUiSP3Bg8"
      },
      "source": [
        "import tensorflow as tf \n",
        "#Use for images convert into array\n",
        "import numpy as np \n",
        "#Use for image preprocessing\n",
        "import cv2\n",
        "# Use for handle data files \n",
        "import os \n",
        "#graphcial Representaion of images data \n",
        "import matplotlib.pyplot as plt\n",
        "# use for rescale images\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Conv2D\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNIe5ZGo3BzT"
      },
      "source": [
        "img = image.load_img(\"/content/drive/MyDrive/new-Classification/basedata/training data/Correct_Position/IMG-20201114-WA0012.jpg\")\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-y_Xx4N3B4u"
      },
      "source": [
        "cv2.imread(\"/content/drive/MyDrive/new-Classification/basedata/training data/Correct_Position/IMG-20201114-WA0012.jpg\").shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTjh_bpl3B8K"
      },
      "source": [
        "train = ImageDataGenerator(rescale=1/255)\n",
        "validation = ImageDataGenerator(rescale=1/255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSOiWgLk3CLU"
      },
      "source": [
        "train_dataset = train.flow_from_directory(\"/content/drive/MyDrive/new-Classification/basedata/training data\" ,\n",
        "                                          target_size = (200 ,200),\n",
        "                                          batch_size = 3,\n",
        "                                          class_mode = (\"binary\")\n",
        "                                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ65U0GD9zUj"
      },
      "source": [
        "validation_dataset = validation.flow_from_directory(\"/content/drive/MyDrive/new-Classification/basedata/validation data\" ,\n",
        "                                          target_size = (200 ,200),\n",
        "                                          batch_size = 3,\n",
        "                                          class_mode = (\"binary\")\n",
        "                                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAFHCFls-QBr"
      },
      "source": [
        "train_dataset.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkK9rVJP-hIZ"
      },
      "source": [
        "train_dataset.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zk1BafWrZXf"
      },
      "source": [
        "## Building a Small Convnet from Scratch to Get to 72% Accuracy\n",
        "\n",
        "The images that will go into our convnet are 200x200 color images (in the next section on Data Preprocessing, we'll add handling to resize all the images to 200x200 before feeding them into the neural network).\n",
        "\n",
        "Let's code up the architecture. We will stack 3 {convolution + relu + maxpooling} modules. Our convolutions operate on 4x3 windows and our maxpooling layers operate on 2x2 windows. Our first convolution extracts 16 filters, the following one extracts 32 filters, and the last one extracts 64 filters.\n",
        "\n",
        "**NOTE**: This is a configuration that is widely used and known to work well for image classification. Also, since we have relatively few training examples using just three convolutional modules keeps the model small, which lowers the risk of overfitting (which we'll explore in more depth in Exercise 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVmPKu6W6XmR"
      },
      "source": [
        "# a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers.\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3) , input_shape= (200,200,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3,3) , input_shape= (200,200,3), kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Conv2D(32, (3,3) , input_shape= (200,200,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3,3), kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(64, (3,3) , input_shape= (200,200,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3,3), kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Conv2D(128, (3,3) , input_shape= (200,200,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3,3), kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Conv2D(256, (3,3), kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "model.add(Conv2D(256, (3,3) , input_shape= (32,32,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CY-8u151Yq8"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer = RMSprop(lr=0.001),\n",
        "              metrics=[\"accuracy\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWIFSo9Z3s56"
      },
      "source": [
        "\n",
        "history = model.fit_generator(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=20,  \n",
        "      epochs=10,\n",
        "      validation_data=validation_dataset,\n",
        "      validation_steps=10,\n",
        "      verbose=2 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR1VIOlutusO"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfTiPMTutOp"
      },
      "source": [
        "**Evaluating Accuracy and Loss for the Model**\n",
        "\n",
        "Let's plot the training/validation accuracy and loss as collected during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjRm2dYuuY1P"
      },
      "source": [
        "# sets for each training epoch\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Retrieve a list of list results on training and validation data\n",
        "# sets for each training epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc508q_Eu3pQ"
      },
      "source": [
        "As you can see, we are overfitting like it's getting out of fashion. Our training accuracy (in blue) gets close to 100% (!) while our validation accuracy (in green) stalls as almost 60%. Our validation loss reaches its minimum after only five epochs.\n",
        "\n",
        "Since we have a relatively small number of training examples, overfitting should be our number one concern. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three images of people who are sailors, and among them the only person wearing a cap is a lumberjack, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.\n",
        "\n",
        "Overfitting is the central problem in machine learning: given that we are fitting the parameters of our model to a given dataset, how can we make sure that the representations learned by the model will be applicable to data never seen before? How do we avoid learning things that are specific to the training data?\n",
        "\n",
        "In the next exercise, we'll look at ways to prevent overfitting in the Correct vs Wrong Chin up classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-ZTjuyH_hk_"
      },
      "source": [
        "path_img = \"/content/drive/My Drive/Classification/basedata/testing data/Correct_Position\"\n",
        "for i in os.listdir(path_img):\n",
        "\n",
        "  img = image.load_img(path_img+'//'+i , target_size=(200 , 200 ,3))\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "  X = image.img_to_array(img)\n",
        "  X = np.expand_dims(X , axis=0)\n",
        "  images = np.vstack([X])\n",
        "  \n",
        "  valid = model.predict(images)\n",
        "  if valid == 0:\n",
        "    print(\"Correct Position\")\n",
        "  else:\n",
        "    print(\"Not Correct Position\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}